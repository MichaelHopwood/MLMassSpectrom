{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ba000c5",
   "metadata": {},
   "source": [
    "# Classify a dynamic length sequence\n",
    "\n",
    "THIS NOTEBOOK IS _OUT DATED_. PLEASE SEE `Classify.py` FOR LATEST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8379ee8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from core import generate_sample_group\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, TimeDistributed\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94938d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_num_nonzero(num_samples, lognorm_mean=3, lognorm_sigm=1):\n",
    "    # Select number of nonzero terms in spectroscopy sample\n",
    "    return np.random.lognormal(lognorm_mean, lognorm_sigm, num_samples).astype(int)\n",
    "\n",
    "\n",
    "def generate_mass_samples(num_nonzero, num_samples,\n",
    "                          low=0, high=300, mass_std=0.001,\n",
    "                          intensity_spread_factor=1.0, intensity_std=0.01):\n",
    "    # Generate the location of the num_nonzero components\n",
    "    #intensity_locs = np.random.dirichlet(np.ones(num_nonzero)*float(intensity_spread_factor))\n",
    "    counts = np.random.uniform(0, 1, num_nonzero)\n",
    "    intensity_locs = counts / counts.sum()\n",
    "\n",
    "    samples = []\n",
    "    for i in range(num_nonzero):\n",
    "        loc = np.random.uniform(low, high)\n",
    "        mass_samples = np.random.normal(loc, mass_std, num_samples)\n",
    "        intensity_samples = np.random.normal(intensity_locs[i], intensity_std, num_samples)\n",
    "        samples.append(np.column_stack((mass_samples, intensity_samples)))\n",
    "    \n",
    "    samples = np.asarray(list(zip(*samples)))\n",
    "    # ToDo: Improve. This is a weak fix.\n",
    "    samples[samples < 0] = 0\n",
    "    return samples\n",
    "\n",
    "def generate_sample_group(num_samples, num_nonzero=None, lognorm_mean=3, lognorm_sigm=1,\n",
    "                         low=0, high=300, mass_std=0.001,\n",
    "                         intensity_spread_factor=1.0, intensity_std=0.01):\n",
    "    if isinstance(num_nonzero, type(None)):\n",
    "        num_nonzero = generate_num_nonzero(1, lognorm_mean=3, lognorm_sigm=1)[0]\n",
    "    samples = generate_mass_samples(num_nonzero, num_samples,\n",
    "                              low=low, high=high, mass_std=mass_std,\n",
    "                              intensity_spread_factor=intensity_spread_factor, intensity_std=intensity_std)\n",
    "    return samples, num_nonzero\n",
    "\n",
    "\n",
    "def generate_all_samples(ngroups=100, nsamples_in_group=10, **kwargs):\n",
    "    \"\"\"Generates samples for multiple test groups.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : `ngroup` size list of arrays: (nsamples_in_group, random_num_nonzero, 2)\n",
    "    y : `ngroup`*`nsamples_in_group` size list of strings\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    lengths = []\n",
    "    for group_i in range(ngroups):\n",
    "        samples, num_nonzero = generate_sample_group(nsamples_in_group, **kwargs)       \n",
    "        X.extend(samples)\n",
    "        y.extend([group_i]*nsamples_in_group)\n",
    "        lengths.extend([num_nonzero]*nsamples_in_group)\n",
    "    return X, np.array(y), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6280bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: 990 First sample in X: (94, 2)\n",
      "y: (1000, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'X has 990 samples, built by 100 groups with 10 samples each. The first group is of size (94, 2), where the first index is the number of nonzero entries and 2 defines the ordered pair (mass and intensity). Y has 1000 definitions each of size 100 to record the one-hot encoded vector.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nsamples_in_group = 10\n",
    "ngroups = 100\n",
    "X, y, lengths = generate_all_samples(ngroups=ngroups, nsamples_in_group=nsamples_in_group, num_nonzero=None)\n",
    "y = to_categorical(y)\n",
    "\n",
    "print(\"X:\", len(X), \"First sample in X:\", X[0].shape)\n",
    "print(\"y:\", y.shape)\n",
    "\n",
    "f\"X has {len(X)} samples, built by {ngroups} groups with {nsamples_in_group} samples each. \"+\\\n",
    "f\"The first group is of size {X[0].shape}, where the first index is the number of nonzero \"+\\\n",
    "f\"entries and 2 defines the ordered pair (mass and intensity). Y has {len(y)} definitions \"+\\\n",
    "f\"each of size {len(y[0])} to record the one-hot encoded vector.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9ebd3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle\n",
    "Xyl = list(zip(X, y, lengths))\n",
    "random.shuffle(Xyl)\n",
    "X, y, lengths = zip(*Xyl)\n",
    "\n",
    "# Split into train-test splits\n",
    "train_pct = 0.9\n",
    "train_split = int(train_pct * len(y))\n",
    "\n",
    "trainX, trainy, trainLengths = X[:train_split], y[:train_split], lengths[:train_split]\n",
    "testX, testy = X[train_split:], y[train_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c54526",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7      71\n",
       "18     38\n",
       "15     34\n",
       "11     32\n",
       "4      29\n",
       "       ..\n",
       "120     8\n",
       "133     8\n",
       "263     8\n",
       "28      7\n",
       "5       7\n",
       "Length: 62, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.Series(trainLengths).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29b8f67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_6 (LSTM)                (None, None, 32)          4480      \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 8)                 1312      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 125)               1125      \n",
      "=================================================================\n",
      "Total params: 6,917\n",
      "Trainable params: 6,917\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(None, 2)))\n",
    "model.add(LSTM(8, return_sequences=False))\n",
    "model.add(Dense(125, activation='sigmoid'))\n",
    "\n",
    "print(model.summary())\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "687a31dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3, 2) (1000, 125)\n",
      "Epoch 1/10\n",
      "(1000, 3, 2) (1000, 125)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Creating variables on a non-first call to a function decorated with tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-9d06400f230a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;31m#model.fit(X,y)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\users\\michaelhopwood\\miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1941\u001b[0m                   \u001b[1;34m'will be removed in a future version. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1942\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[1;32m-> 1943\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1944\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1945\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\users\\michaelhopwood\\miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\users\\michaelhopwood\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\users\\michaelhopwood\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    924\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 926\u001b[1;33m         raise ValueError(\"Creating variables on a non-first call to a function\"\n\u001b[0m\u001b[0;32m    927\u001b[0m                          \" decorated with tf.function.\")\n\u001b[0;32m    928\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."
     ]
    }
   ],
   "source": [
    "# def train_generator():\n",
    "#     global trainX, trainy, trainLengths\n",
    "#     trainyarr = np.array(trainy)\n",
    "#     while True:\n",
    "#         sequence_length = np.random.choice(trainLengths)\n",
    "#         #TODO: is seeing same data multiple times\n",
    "#         mask = np.array(trainLengths)==sequence_length\n",
    "#         #x_train = trainX[mask, :, :]\n",
    "#         #y_train = trainy[mask, :]\n",
    "#         arrs = []\n",
    "#         for b,arr in zip(mask,trainX):\n",
    "#             if b:\n",
    "#                 arrs.append(arr)\n",
    "                \n",
    "#         #x_train = np.asarray(trainX)[mask]\n",
    "#         x_train = np.asarray(arrs)\n",
    "# #         for i in range(len(x_train)):\n",
    "# #             print(x_train[i].shape, type(x_train[i]))\n",
    "# #         print(x_train.shape)\n",
    "#         #print(x_train.shape, x_train[0].shape, x_train[-1].shape)\n",
    "#         y_train = trainyarr[mask]\n",
    "#         print(x_train.shape, y_train.shape, type(x_train), type(y_train))\n",
    "        \n",
    "#         yield x_train, y_train\n",
    "\n",
    "# def train_generator():\n",
    "#     while True:\n",
    "#         sequence_length = np.random.randint(10, 100)\n",
    "#         x_train = np.random.random((1000, sequence_length, 5))\n",
    "#         # y_train will depend on past 5 timesteps of x\n",
    "#         y_train = x_train[:, :, 0]\n",
    "#         for i in range(1, 5):\n",
    "#             y_train[:, i:] += x_train[:, :-i, i]\n",
    "#         y_train = to_categorical(y_train > 2.5)\n",
    "#         print(x_train.shape, y_train.shape)\n",
    "#         yield x_train, y_train\n",
    "\n",
    "testX, testY = [], []\n",
    "def train_generator():\n",
    "    global testX, testY\n",
    "\n",
    "    while True:\n",
    "        sequence_length = generate_num_nonzero(1, lognorm_mean=3, lognorm_sigm=1)[0]\n",
    "        sequence_length=3\n",
    "        generate_num_nonzero\n",
    "        nsamples_in_group = 10\n",
    "        ngroups = 125\n",
    "        X, y, lengths = generate_all_samples(ngroups=ngroups,\n",
    "                                             nsamples_in_group=nsamples_in_group,\n",
    "                                             num_nonzero=sequence_length)\n",
    "        X = np.asarray(X)\n",
    "        y = to_categorical(y)\n",
    "\n",
    "        # Shuffle\n",
    "        Xyl = list(zip(X, y, lengths))\n",
    "        random.shuffle(Xyl)\n",
    "        X, y, lengths = zip(*Xyl)\n",
    "        X=np.asarray(X)\n",
    "        y=np.asarray(y)\n",
    "\n",
    "        # Split into train-test splits\n",
    "        train_pct = 0.9\n",
    "        train_split = 1000#int(train_pct * len(y))\n",
    "\n",
    "        trainX, trainy, trainLengths = X[:train_split], y[:train_split], lengths[:train_split]\n",
    "        testX.extend(X[train_split:])\n",
    "        testY.extend(y[train_split:])\n",
    "        print(trainX.shape, trainy.shape)\n",
    "        yield trainX, trainy\n",
    "\n",
    "\n",
    "model.fit_generator(train_generator(), steps_per_epoch=10, epochs=10, verbose=1)\n",
    "#model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d245ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
